{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport re\nimport urllib\nimport random\nimport nltk\nnltk.download('words')\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \nimport numpy as np\nimport pickle\n\nrandom.seed(42)","metadata":{"id":"Pzvh53TF7CYc","outputId":"8be95872-bfb3-46c7-cbec-3c19d0bb2723","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = \"https://raw.githubusercontent.com/nee2shaji/IIIT_Sem4/main/brown.txt\"\nresponse = urllib.request.urlopen(data_path)\nbrowntext = response.read()\nbrowntext = browntext.decode(\"utf-8\")\nbrowntext = re.sub(r'[\\r\\n]+', '', browntext)\nbrowntext = browntext.split('.')\n\nfor i in range(0, len(browntext)):\n    browntext[i] = re.sub(r'[0-9_]+', '', browntext[i])\n    browntext[i] = re.sub(r'n\\'t', 'nt', browntext[i])\n    browntext[i] = re.sub(r'\\'ll', ' will', browntext[i])\n    browntext[i] = re.sub(r'\\'ve', ' have', browntext[i])\n    browntext[i] = re.sub(r'\\'re', ' are', browntext[i])\n    browntext[i] = re.sub(r' i\\'m ', ' i am ', browntext[i])\n    browntext[i] = re.sub(r'\\'s', 's', browntext[i])\n    browntext[i] = re.sub(r'[^\\w\\s]', ' ', browntext[i])\n    browntext[i] = re.sub(r'[\\s]+', ' ', browntext[i])\n    browntext[i] = re.sub(r'^ ', '', browntext[i])\n    browntext[i] = browntext[i].lower()\n\n# Check if in english dict\nwords = set(nltk.corpus.words.words())\nfor i in range(0, len(browntext)):\n  browntext[i] = \" \".join([ w for w in browntext[i].split() if w in words ])\n\ndummy = (' ').join(browntext)\nreq = nltk.FreqDist(dummy.split())\nfor i in range(0, len(browntext)):\n  browntext[i] = \" \".join([ w for w in browntext[i].split() if req[w] > 20 ])\n\n# lemmatizer = WordNetLemmatizer()\n# for i in range(0, len(browntext)):\n#     browntext[i] = \" \".join([lemmatizer.lemmatize(w) for w in browntext[i].split() ])\n\n# shuffle sentences and split into train test and validation\nrandom.shuffle(browntext)\nbrowntext_len = len(browntext)\ni = round(browntext_len*0.5)\nj = round(browntext_len*0.8)\ntrain_set = browntext[0:i]\nvalidation_set = browntext[i:j]\ntest_set = browntext[j:]\nprint(len(browntext), len(train_set), len(test_set), len(validation_set), i, j)\n","metadata":{"id":"6uAMN_hb7EyW","outputId":"2dd344e8-7c49-4fba-8dd6-e3267f3b4b64","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy = (' ').join(train_set)\n\n# print(len(set(dummy.split())))\n\n# words = set(nltk.corpus.words.words())\n\n# print(len([ w for w in set(dummy.split()) if(w in words)] ))\n# req = nltk.FreqDist(dummy.split())\n# i=0\n# for k,v in req.items():\n#   if(v==2):\n#     print(str(k) + ': ' + str(v))\n#     i=i+1\n# print (i)\n# print(len([ w for w in set(dummy.split()) if(w in words)] ))","metadata":{"id":"f1I8tOUe8UD1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_set)\ntotal_vocab = len(tokenizer.word_index) + 1\nprint(total_vocab)\n\ninput_sequences = []\nfor line in train_set:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        input_sequences.append(token_list[:i+1])\n","metadata":{"id":"ZbYQ1fUoqp85","outputId":"07b5aa6e-5a79-4330-9af5-710e81d20e8d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","metadata":{"id":"gxmt4AwfxsvB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, train_y = input_sequences[:,:-1],input_sequences[:,-1]\ntrain_y = ku.to_categorical(train_y, num_classes=total_vocab)\n","metadata":{"id":"aZmNdMCSxxTq","outputId":"b55118f4-424a-4ea1-abca-7090b7bee1d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(train_X, train_y, max_sequence_len, total_vocab):\n    model = Sequential()\n    model.add(Embedding(total_vocab, 10, input_length=max_sequence_len))\n    model.add(LSTM(128, return_sequences=True))\n    model.add(LSTM(128))\n    model.add(Dropout(0.2))\n    model.add(Dense(total_vocab, activation='softmax'))    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n    print(model.summary())\n    model.fit(train_X, train_y, epochs=10, verbose=1)\n    return model\n#     model = Sequential()\n#   model.add(Embedding(total_vocab, 10, input_length=input_len))\n#   model.add(LSTM(10))\n#   model.add(Dropout(0.1))\n#   model.add(Dense(1, activation='softmax'))\n#   model.compile(loss='categorical_crossentropy', optimizer='adam')\n#   model.fit(train_X, train_y, epochs=5, verbose=1)","metadata":{"id":"jzzVF2TwxTwf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(train_X, train_y, max_sequence_len, total_vocab)","metadata":{"id":"XFCBytVhFDlo","outputId":"ca14360c-645b-41a0-83fb-1fc941638dd7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}